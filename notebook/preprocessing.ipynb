{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4a33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "#import json\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61463f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirpath = '../data'\n",
    "cvs_filepath = data_dirpath + '/resumes_we2_edu1_about100.txt.zip'\n",
    "embedding_filepath = data_dirpath + '/glove_s300.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1f1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_tokens(embedding_filepath):\n",
    "    z = zipfile.ZipFile(embedding_filepath, 'r')\n",
    "    if 'glove_s300.txt' not in z.namelist():\n",
    "        return []\n",
    "    \n",
    "    # Read line-by-line due to filesize (~3gb in raw text)\n",
    "    valid_tokens = set()\n",
    "    for row, line in enumerate(z.open('glove_s300.txt')):\n",
    "        line = line.decode('utf-8')\n",
    "        if row == 0: # HEADER\n",
    "            num_embeddings, embedding_size = line.split(' ')\n",
    "        else:\n",
    "            token_name, embedding_str = line.split(' ', 1)\n",
    "            valid_tokens.add(token_name)\n",
    "    z.close()\n",
    "    return valid_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e591678",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = get_valid_tokens(embedding_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b55415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE:\n",
    "# # https://github.com/nathanshartmann/portuguese_word_embeddings/blob/master/preprocessing.py\n",
    "\n",
    "# Punctuation list\n",
    "punctuations = re.escape('!\"#%\\'()*+,./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "# ##### #\n",
    "# Regex #\n",
    "# ##### #\n",
    "re_remove_brackets = re.compile(r'\\{.*\\}')\n",
    "re_remove_html = re.compile(r'<(\\/|\\\\)?.+?>', re.UNICODE)\n",
    "re_transform_numbers = re.compile(r'\\d', re.UNICODE)\n",
    "re_transform_emails = re.compile(r'[^\\s]+@[^\\s]+', re.UNICODE)\n",
    "re_transform_url = re.compile(r'(http|https)://[^\\s]+', re.UNICODE)\n",
    "# Different quotes are used.\n",
    "re_quotes_1 = re.compile(r\"(?u)(^|\\W)[‘’′`']\", re.UNICODE)\n",
    "re_quotes_2 = re.compile(r\"(?u)[‘’`′'](\\W|$)\", re.UNICODE)\n",
    "re_quotes_3 = re.compile(r'(?u)[‘’`′“”]', re.UNICODE)\n",
    "re_dots = re.compile(r'(?<!\\.)\\.\\.(?!\\.)', re.UNICODE)\n",
    "re_punctuation = re.compile(r'([,\";:]){2},', re.UNICODE)\n",
    "re_hiphen = re.compile(r' -(?=[^\\W\\d_])', re.UNICODE)\n",
    "re_tree_dots = re.compile(u'…', re.UNICODE)\n",
    "# Differents punctuation patterns are used.\n",
    "re_punkts = re.compile(r'(\\w+)([%s])([ %s])' %\n",
    "                       (punctuations, punctuations), re.UNICODE)\n",
    "re_punkts_b = re.compile(r'([ %s])([%s])(\\w+)' %\n",
    "                         (punctuations, punctuations), re.UNICODE)\n",
    "re_punkts_c = re.compile(r'(\\w+)([%s])$' % (punctuations), re.UNICODE)\n",
    "re_changehyphen = re.compile(u'–')\n",
    "re_doublequotes_1 = re.compile(r'(\\\"\\\")')\n",
    "re_doublequotes_2 = re.compile(r'(\\'\\')')\n",
    "re_trim = re.compile(r'\\s+', re.UNICODE)\n",
    "\n",
    "def tokenize_text(text, token_set):\n",
    "    \"\"\"Apply all regex above to a given string.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\r?\\n', ' ', text)\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re_tree_dots.sub('...', text)\n",
    "    text = re.sub('\\.\\.\\.', '', text)\n",
    "    text = re_remove_brackets.sub('', text)\n",
    "    text = re_changehyphen.sub('-', text)\n",
    "    text = re_remove_html.sub(' ', text)\n",
    "    text = re_transform_numbers.sub('0', text)\n",
    "    text = re_transform_url.sub('URL', text)\n",
    "    text = re_transform_emails.sub('EMAIL', text)\n",
    "    text = re_quotes_1.sub(r'\\1\"', text)\n",
    "    text = re_quotes_2.sub(r'\"\\1', text)\n",
    "    text = re_quotes_3.sub('\"', text)\n",
    "    text = re.sub('\"', '', text)\n",
    "    text = re_dots.sub('.', text)\n",
    "    text = re_punctuation.sub(r'\\1', text)\n",
    "    text = re_hiphen.sub(' - ', text)\n",
    "    text = re_punkts.sub(r'\\1 \\2 \\3', text)\n",
    "    text = re_punkts_b.sub(r'\\1 \\2 \\3', text)\n",
    "    text = re_punkts_c.sub(r'\\1 \\2', text)\n",
    "    text = re_doublequotes_1.sub('\\\"', text)\n",
    "    text = re_doublequotes_2.sub('\\'', text)\n",
    "    text = re_trim.sub(' ', text)\n",
    "    text = text.strip()\n",
    "    return [word if not token_set or word in token_set else '<unk>'\n",
    "            for word in text.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dab67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = '%Y-%m-%dT%H:%M:%S%z'\n",
    "def format_date(date_str):\n",
    "    date = datetime.strptime(date_str, date_format)\n",
    "    date_template = '0{}/{}' if date.month < 10 else '{}/{}'\n",
    "    return date_template.format(date.month, date.year)\n",
    "\n",
    "cv_tokens_set = set()\n",
    "def tokenize_cv(cv_data, token_set=[]):\n",
    "    if isinstance(cv_data, str):\n",
    "        tokens = tokenize_text(cv_data, token_set)\n",
    "        cv_tokens_set.update(tokens)\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    elif isinstance(cv_data, list):\n",
    "        return [tokenize_cv(entry, token_set) for entry in cv_data]\n",
    "    \n",
    "    elif isinstance(cv_data, dict):\n",
    "        processed_cv = dict(cv_data)\n",
    "        for key in processed_cv.keys():\n",
    "            if key in ['dateInit', 'dateEnd']:\n",
    "                processed_cv[key] = format_date(processed_cv[key])\n",
    "            processed_cv[key] = tokenize_cv(processed_cv[key], token_set)\n",
    "        return processed_cv\n",
    "    elif isinstance(cv_data, int):\n",
    "        return cv_data\n",
    "    elif not cv_data:\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError('Type not mapped: ' + str(type(cv_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d3340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n"
     ]
    }
   ],
   "source": [
    "cv_tokens_set = set()\n",
    "\n",
    "cvs = []\n",
    "with gzip.open(cvs_filepath, mode='rt', encoding='utf8') as z:\n",
    "    for i, line in enumerate(z):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        cv = json.loads(line)\n",
    "        cv = tokenize_cv(cv, token_set)\n",
    "        cvs.append(json.dumps(cv))\n",
    "\n",
    "# After save file, replace '.txt' by '.txt.zip'\n",
    "with gzip.open(data_dirpath + '/resumes_we2_edu1_about100_[preprocessed].txt', mode='w') as zipfile:\n",
    "    zipfile.write('\\n'.join(cvs).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3ca4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dirpath + '/resumes_we2_edu1_about100_[tokens].txt', 'w', encoding='utf-8') as fp:\n",
    "    for token in cv_tokens_set:        \n",
    "        fp.write(token + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
